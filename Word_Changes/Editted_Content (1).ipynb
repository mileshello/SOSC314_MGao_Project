{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import difflib\n",
        "import csv\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "#parameters\n",
        "\n",
        "URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"WikiEditStudy/1.0 (student project)\"\n",
        "}\n",
        "\n",
        "PARAMS = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"prop\": \"revisions\",\n",
        "    \"titles\": \"Legal status of transgender people\",\n",
        "    \"rvprop\": \"content\",\n",
        "    \"rvslots\": \"main\",\n",
        "    \"rvstart\": \"2023-12-31T23:59:59Z\",\n",
        "    \"rvend\": \"2022-01-01T00:00:00Z\",\n",
        "    \"rvlimit\": \"max\",\n",
        "    \"rvdir\": \"older\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "uMHUf1rlg0WL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a747b2-869f-474c-f35c-8bb1530c9614"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved wiki_word_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Lowercase and extract alphabetic words.\"\"\"\n",
        "    return re.findall(r\"\\b[a-zA-Z]+\\b\", text.lower())"
      ],
      "metadata": {
        "id": "Gx2m664yi_t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#edits finder\n",
        "\n",
        "def get_revisions():\n",
        "    revisions = []\n",
        "    cont = {}\n",
        "\n",
        "    while True:\n",
        "        params = PARAMS.copy()\n",
        "        params.update(cont)\n",
        "\n",
        "        r = requests.get(URL, params=params, headers=HEADERS)\n",
        "        data = r.json()\n",
        "\n",
        "        pages = data[\"query\"][\"pages\"]\n",
        "        page = next(iter(pages.values()))\n",
        "\n",
        "        for rev in page[\"revisions\"]:\n",
        "            slot = rev[\"slots\"][\"main\"]\n",
        "            text = slot.get(\"*\", slot.get(\"content\", \"\"))\n",
        "            if text:\n",
        "                revisions.append(text)\n",
        "\n",
        "        if \"continue\" in data:\n",
        "            cont = data[\"continue\"]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return revisions"
      ],
      "metadata": {
        "id": "Lajsew4g0yAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    revisions = get_revisions()\n",
        "\n",
        "    added_counts = defaultdict(int)\n",
        "    removed_counts = defaultdict(int)\n",
        "\n",
        "    for i in range(len(revisions) - 1):\n",
        "        old_words = tokenize(revisions[i])\n",
        "        new_words = tokenize(revisions[i + 1])\n",
        "\n",
        "        diff = difflib.ndiff(old_words, new_words)\n",
        "\n",
        "        for d in diff:\n",
        "            word = d[2:]\n",
        "            if d.startswith(\"+ \"):\n",
        "                added_counts[word] += 1\n",
        "            elif d.startswith(\"- \"):\n",
        "                removed_counts[word] += 1\n",
        "\n",
        "    # Merge counts and compute net\n",
        "    all_words = set(list(added_counts.keys()) + list(removed_counts.keys()))\n",
        "    results = []\n",
        "\n",
        "    for word in all_words:\n",
        "        added = added_counts.get(word, 0)\n",
        "        removed = removed_counts.get(word, 0)\n",
        "        net = added - removed\n",
        "        results.append({\n",
        "            \"word\": word,\n",
        "            \"times_added\": added,\n",
        "            \"times_removed\": removed,\n",
        "            \"net_change\": net\n",
        "        })\n",
        "\n",
        "    # Sort by times_added descending\n",
        "    results.sort(key=lambda x: x[\"times_added\"], reverse=True)\n",
        "\n",
        "    # Write CSV\n",
        "    with open(\"wiki_word_summary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"word\", \"times_added\", \"times_removed\", \"net_change\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(\"Saved wiki_word_summary.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "l88xBXzl06Ff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}